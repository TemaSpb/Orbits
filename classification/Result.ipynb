{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import *\n",
    "import glob\n",
    "from keras.layers import Activation, Conv2D, Dropout, MaxPooling2D, Flatten, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plots(ims, figsize=(12, 6), rows = 1):\n",
    "    if type(ims[0]) is np.ndarray:\n",
    "        ims = np.array(ims)\n",
    "        if (ims.shape[-1] != 3):\n",
    "            ims = ims.transpose((0, 2, 3, 1))\n",
    "    f = plt.figure(figsize = figsize)\n",
    "    cols = len(ims) // rows if len(ims) % 2 == 0 else len(ims) // rows + 1\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, cols, i+1)\n",
    "        sp.axis('Off')\n",
    "        plt.imshow(ims[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_generator(data_dir, batch_size, img_width, img_height,\n",
    "                     rotation_range = 0, width_shift_range = 0,\n",
    "                     height_shift_range = 0, shear_range = 0,\n",
    "                     zoom_range = 0, horizontal_flip = False):\n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale = 1./255, rotation_range = rotation_range, width_shift_range = width_shift_range,\n",
    "        height_shift_range = height_shift_range, shear_range = shear_range, zoom_range = zoom_range,\n",
    "        horizontal_flip = horizontal_flip)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        data_dir, target_size = (img_width, img_height),\n",
    "        batch_size = batch_size, class_mode = 'binary')\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data augmentation example\n",
    "train_generator = create_generator(train_data_dir, batch_size,\n",
    "                                       img_width, img_height, **generator_params)\n",
    "aug_images = [next(train_generator)[0][0] for i in range(10)]\n",
    "plots(aug_images, figsize=(20,7), rows=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(img_width, img_height):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, 11 ,activation = 'relu', input_shape = (img_width, img_height, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "\n",
    "    model.add(Conv2D(64, 3 ,activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, 3 ,activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, 3 ,activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation = 'relu'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "        optimizer='rmsprop',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logs(path, name, accuracy, img_width, img_height,\n",
    "                epochs, batch_size, data_augmentation = False):\n",
    "    info = (\"number of epochs = {epochs}\" + \"\\nbatch_size = \" +\n",
    "            str(batch_size) + \"\\nimage size = (\" + str(img_width) + \",\" +\n",
    "            str(img_height) + \")\\n\" + \"data augmentation = \" +\n",
    "            str(data_augmentation) + \"\\naccuracy = \" + str(accuracy) + \"\\n\")\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"    for metric in evaluation:\n",
    "        info += metric + \" : \"\n",
    "    with open(path + \"/\" + name + \".txt\" , \"w\") as text_file:\n",
    "        text_file.write(info)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of epochs = {epochs}\n",
      "batch_size = 4\n",
      "image size = (512,512)\n",
      "data augmentation = False\n",
      "accuracy = 3.123123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(create_logs(\"Asd\", \"Asd\", 3.123123, img_width, img_height,\n",
    "                epochs, batch_size, data_augmentation = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.22579369739426625, 0.9166666666666666]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator, nb_validation_samples/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_width = 512\n",
    "img_height = 512\n",
    "\n",
    "train_data_dir = 'dataset_classification/1/train'\n",
    "validation_data_dir = 'dataset_classification/1/validation'\n",
    "nb_train_samples = 182 #1160  \n",
    "nb_validation_samples = 48 #370\n",
    "epochs = 40\n",
    "batch_size = 4\n",
    "without_augmentation = True\n",
    "generator_params = {\"rotation_range\" : 10, \"width_shift_range\" : 0.2,\n",
    "                    \"height_shift_range\" : 0.2, \"shear_range\" : 0.2,\n",
    "                    \"zoom_range\" : 0.3, \"horizontal_flip\" : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_width = 256\n",
    "img_height = 256\n",
    "\n",
    "train_data_dir = 'dataset_classification/2/train'\n",
    "validation_data_dir = 'dataset_classification/2/validation'\n",
    "nb_train_samples = 182# 1160  \n",
    "nb_validation_samples = 48#370\n",
    "epochs = 30\n",
    "batch_size = 4\n",
    "without_augmentation = True\n",
    "generator_params = {\"rotation_range\" : 5, \"width_shift_range\" : 0.1,\n",
    "                    \"height_shift_range\" : 0.1, \"shear_range\" : 0.1,\n",
    "                    \"zoom_range\" : 0.2, \"horizontal_flip\" : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 182 images belonging to 2 classes.\n",
      "Found 48 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "45/45 [==============================] - 51s 1s/step - loss: 1.5595 - acc: 0.5724 - val_loss: 0.6612 - val_acc: 0.5417\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 55s 1s/step - loss: 0.7193 - acc: 0.7222 - val_loss: 0.4844 - val_acc: 0.7917\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 54s 1s/step - loss: 0.3762 - acc: 0.8222 - val_loss: 0.3850 - val_acc: 0.7292\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 54s 1s/step - loss: 0.3230 - acc: 0.8722 - val_loss: 0.2894 - val_acc: 0.9167\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.3268 - acc: 0.9166 - val_loss: 0.3087 - val_acc: 0.9167\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.2872 - acc: 0.8944 - val_loss: 0.5543 - val_acc: 0.8333\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 54s 1s/step - loss: 0.1425 - acc: 0.9667 - val_loss: 0.8459 - val_acc: 0.8125\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.1088 - acc: 0.9667 - val_loss: 0.6162 - val_acc: 0.8958\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.2486 - acc: 0.9444 - val_loss: 0.1913 - val_acc: 0.9375\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 4.4864 - acc: 0.7166 - val_loss: 7.3875 - val_acc: 0.5417\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 5.6413 - acc: 0.6500 - val_loss: 7.3875 - val_acc: 0.5417\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 52s 1s/step - loss: 5.8173 - acc: 0.6391 - val_loss: 7.3875 - val_acc: 0.5417\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 6.2677 - acc: 0.6111 - val_loss: 7.3875 - val_acc: 0.5417\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 5.7309 - acc: 0.6444 - val_loss: 7.3875 - val_acc: 0.5417\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 5.9084 - acc: 0.6332 - val_loss: 1.5244 - val_acc: 0.8750\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 52s 1s/step - loss: 0.2840 - acc: 0.9501 - val_loss: 0.3643 - val_acc: 0.9167\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.1534 - acc: 0.9611 - val_loss: 0.3307 - val_acc: 0.9375\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.4625 - acc: 0.9389 - val_loss: 0.2159 - val_acc: 0.9375\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.1194 - acc: 0.9722 - val_loss: 0.1358 - val_acc: 0.9375\n",
      "Epoch 20/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.0390 - acc: 0.9833 - val_loss: 0.3880 - val_acc: 0.9583\n",
      "Epoch 21/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.3652 - acc: 0.9611 - val_loss: 0.1346 - val_acc: 0.9375\n",
      "Epoch 22/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.0324 - acc: 0.9889 - val_loss: 0.2232 - val_acc: 0.9375\n",
      "Epoch 23/30\n",
      "45/45 [==============================] - 52s 1s/step - loss: 0.2904 - acc: 0.9501 - val_loss: 0.7743 - val_acc: 0.9167\n",
      "Epoch 24/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.1286 - acc: 0.9833 - val_loss: 1.0404 - val_acc: 0.9167\n",
      "Epoch 25/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.3804 - acc: 0.9444 - val_loss: 0.1427 - val_acc: 0.9167\n",
      "Epoch 26/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.0927 - acc: 0.9722 - val_loss: 0.0460 - val_acc: 0.9792\n",
      "Epoch 27/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.1567 - acc: 0.9667 - val_loss: 0.2405 - val_acc: 0.9375\n",
      "Epoch 28/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.1674 - acc: 0.9667 - val_loss: 0.1018 - val_acc: 0.9375\n",
      "Epoch 29/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.0218 - acc: 0.9889 - val_loss: 0.8256 - val_acc: 0.8750\n",
      "Epoch 30/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 0.1816 - acc: 0.9667 - val_loss: 0.2258 - val_acc: 0.9167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cd7c26a278>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(img_width, img_height)\n",
    "\n",
    "if without_augmentation:\n",
    "    train_generator = create_generator(train_data_dir, batch_size,\n",
    "                                       img_width, img_height)\n",
    "else:\n",
    "    train_generator = create_generator(train_data_dir, batch_size,\n",
    "                                       img_width, img_height, **generator_params)\n",
    "    \n",
    "validation_generator = create_generator(validation_data_dir, batch_size, img_width, img_height)\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator, steps_per_epoch = nb_train_samples // batch_size,\n",
    "        epochs = epochs, validation_data = validation_generator,\n",
    "        validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dict_with_params(path):\n",
    "    param_list = open(path, 'r').read().split('\\n')\n",
    "    param_list = list(map(lambda x: x.split(\" = \"), param_list))\n",
    "    d = {x[0]:x[1] for x in param_list}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': '4',\n",
       " 'epochs': '30',\n",
       " 'generator_params': '{rotation_range : 5, width_shift_range : 0.1, height_shift_range : 0.1, shear_range : 0.1, zoom_range : 0.2, horizontal_flip : True}',\n",
       " 'img_height': '256',\n",
       " 'img_width': '256',\n",
       " 'nb_train_samples': '500',\n",
       " 'nb_validation_samples': '48',\n",
       " 'train_data_dir': 'data/256/train',\n",
       " 'validation_data_dir': 'data/256/validation',\n",
       " 'without_augmentation': 'False'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dict_with_params(\"cfg/cfg4.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
